{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load stuff we will need\n",
    "import pandas as pd\n",
    "import kinecture\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# When we make changes to modules, automatically reload the modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and generate features\n",
    "1. Load data, clean, make features.\n",
    "2. Convert data to a form sklearn understands: X (observations), y (labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train instances: 8906\n",
      "test09 instances: 1108\n",
      "test10 instances: 1068\n",
      "test11 instances: 1149\n"
     ]
    }
   ],
   "source": [
    "# load data, generate features\n",
    "xls = pd.ExcelFile('data/allData-filtered.xlsx')\n",
    "train_dataframe = xls.parse('filtered-noFormulas')\n",
    "test09_dataframe = xls.parse('09test')\n",
    "test10_dataframe = xls.parse('10test')\n",
    "test11_dataframe = xls.parse('11test')\n",
    "\n",
    "print(\"train instances:\", len(train_dataframe))\n",
    "print(\"test09 instances:\", len(test09_dataframe))\n",
    "print(\"test10 instances:\", len(test10_dataframe))\n",
    "print(\"test11 instances:\", len(test11_dataframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate features (determined from data_exploration)\n",
    "train_features = kinecture.gen_features(train_dataframe)\n",
    "test09_features = kinecture.gen_features(test09_dataframe)\n",
    "test10_features = kinecture.gen_features(test10_dataframe)\n",
    "test11_features = kinecture.gen_features(test11_dataframe)\n",
    "\n",
    "test09_features['Truth'] = kinecture.remap_labels({\"SI-FIX\": \"SI\"}, \n",
    "                                                 test09_features['Truth'])\n",
    "test10_features['Truth'] = kinecture.remap_labels({\"SI-FIX\": \"SI\"}, \n",
    "                                                 test10_features['Truth'])\n",
    "test11_features['Truth'] = kinecture.remap_labels({\"SI-FIX\": \"SI\"}, \n",
    "                                                 test11_features['Truth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['duration', 'angleLeft', 'angleRight', 'confidenceLeft',\n",
      "       'confidenceRight', 'loudnessLeft', 'loudnessRight', 'silenceLeft',\n",
      "       'silenceRight', 'speakerX', 'speakerY', 'confLangle', 'confRangle',\n",
      "       'pLeft', 'pRight', 'qLeft', 'qRight', 'loudnessAve', 'loudnessDiff',\n",
      "       'pLeftZ', 'pRightZ', 'qLeftZ', 'qRightZ', 'logLoud', 'logLoudDiff',\n",
      "       'logLoudZ', 'logLoudDiffZ', 'logLoudnessLeft', 'logLoudnessRight',\n",
      "       'averageLogLoudness', 'silenceLeftAndRight', 'silenceLeftOrRight',\n",
      "       'speakerXYNorm', 'speakerXIs0', 'sinAngleLeft', 'sinAngleRight',\n",
      "       'Truth'],\n",
      "      dtype='object')\n",
      "37 features\n"
     ]
    }
   ],
   "source": [
    "print(train_features.columns)\n",
    "print(\"{} features\".format(len(train_features.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert data into a form sklearn likes\n",
    "train_sklearn = kinecture.convert_features_for_sklearn(train_features)\n",
    "test09_sklearn = kinecture.convert_features_for_sklearn(test09_features)\n",
    "test10_sklearn = kinecture.convert_features_for_sklearn(test10_features)\n",
    "test11_sklearn = kinecture.convert_features_for_sklearn(test11_features)\n",
    "\n",
    "testall_sklearn = test09_sklearn.append(test10_sklearn).append(test11_sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a linear classifier\n",
    "First I start by building a classifier using a simple linear decision boundary. \n",
    "\n",
    "I adjusted the importance of each class to be inversely proportional to the class frequency (`class_weight: auto`). This ensures that recall for all classes will be roughly equal, even though there are many more TA and SI instances than S instances. I could also set just the S class to be more important by setting `class_weights={0:3}`, this will make class 0 (S) 3 times more important than SI or TA.\n",
    "\n",
    "Results for all test sessions are as follows:\n",
    "\n",
    "\n",
    "             S   SI   TA\n",
    "        S  669  232  147\n",
    "       SI  300  624  195\n",
    "       TA  139  397  622\n",
    "\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          S       0.60      0.64      0.62      1048\n",
    "         SI       0.50      0.56      0.53      1119\n",
    "         TA       0.65      0.54      0.59      1158\n",
    "      total       0.58      0.58      0.58      3325\n",
    "\n",
    "We see an average recall of 58% which is not good, suggesting better features are needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params for LinearSVC\n",
      "{'C': 1}\n",
      "\n",
      "results for train\n",
      "             S   SI   TA\n",
      "        S  627  457  362\n",
      "       SI  392 1909  932\n",
      "       TA  206  844 3177\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          S       0.51      0.43      0.47      1446\n",
      "         SI       0.59      0.59      0.59      3233\n",
      "         TA       0.71      0.75      0.73      4227\n",
      "\n",
      "avg / total       0.64      0.64      0.64      8906\n",
      "\n",
      "results for test09\n",
      "             S   SI   TA\n",
      "        S  152   29   89\n",
      "       SI   61  146  132\n",
      "       TA   48  195  256\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          S       0.58      0.56      0.57       270\n",
      "         SI       0.39      0.43      0.41       339\n",
      "         TA       0.54      0.51      0.52       499\n",
      "\n",
      "avg / total       0.50      0.50      0.50      1108\n",
      "\n",
      "results for test10\n",
      "             S   SI   TA\n",
      "        S  218   12    3\n",
      "       SI  118  299   56\n",
      "       TA   22  103  237\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          S       0.61      0.94      0.74       233\n",
      "         SI       0.72      0.63      0.67       473\n",
      "         TA       0.80      0.65      0.72       362\n",
      "\n",
      "avg / total       0.72      0.71      0.70      1068\n",
      "\n",
      "results for test11\n",
      "             S   SI   TA\n",
      "        S  299  191   55\n",
      "       SI  121  179    7\n",
      "       TA   69   99  129\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          S       0.61      0.55      0.58       545\n",
      "         SI       0.38      0.58      0.46       307\n",
      "         TA       0.68      0.43      0.53       297\n",
      "\n",
      "avg / total       0.57      0.53      0.53      1149\n",
      "\n",
      "results for testall\n",
      "             S   SI   TA\n",
      "        S  669  232  147\n",
      "       SI  300  624  195\n",
      "       TA  139  397  622\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          S       0.60      0.64      0.62      1048\n",
      "         SI       0.50      0.56      0.53      1119\n",
      "         TA       0.65      0.54      0.59      1158\n",
      "\n",
      "avg / total       0.58      0.58      0.58      3325\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# use grid search to find best hyperparameters\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'C': [0.1, 1, 10]}]\n",
    "\n",
    "# build LinearSVC classifier using training data\n",
    "classifier = GridSearchCV(svm.LinearSVC(class_weight='auto'), tuned_parameters)\n",
    "classifier.fit(train_sklearn.X, train_sklearn.y)\n",
    "\n",
    "print(\"Best params for LinearSVC\")\n",
    "print(classifier.best_params_)\n",
    "print()\n",
    "\n",
    "train_predictions = classifier.predict(train_sklearn.X)\n",
    "kinecture.report_accuracy(train_predictions, train_sklearn.y, header=\"results for train\")\n",
    "\n",
    "test09_predictions = classifier.predict(test09_sklearn.X)\n",
    "kinecture.report_accuracy(test09_predictions, test09_sklearn.y, header=\"results for test09\")\n",
    "\n",
    "test10_predictions = classifier.predict(test10_sklearn.X)\n",
    "kinecture.report_accuracy(test10_predictions, test10_sklearn.y, header=\"results for test10\")\n",
    "\n",
    "test11_predictions = classifier.predict(test11_sklearn.X)\n",
    "kinecture.report_accuracy(test11_predictions, test11_sklearn.y, header=\"results for test11\")\n",
    "\n",
    "testall_predictions = classifier.predict(testall_sklearn.X)\n",
    "kinecture.report_accuracy(testall_predictions, testall_sklearn.y, header=\"results for testall\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a classifiers with nonlinear kernels\n",
    "Since we observed that, at least for pairs of variables, features were not linearly seperable, a more complex decision boundary may perform better.\n",
    "\n",
    "Results for the test were as follows:\n",
    "\n",
    "             S   SI   TA\n",
    "        S  545  350  153\n",
    "       SI  219  711  189\n",
    "       TA  162  380  616\n",
    "\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          S       0.59      0.52      0.55      1048\n",
    "         SI       0.49      0.64      0.56      1119\n",
    "         TA       0.64      0.53      0.58      1158\n",
    "\n",
    "      total       0.58      0.56      0.56      3325\n",
    "      \n",
    "With the nonlinear classifier we see slightly lower recall, suggesting some overfitting to the training data.\n",
    "\n",
    "Overall, my suggestion would be to focus on improving the features, since increasing the model complexity does not seem to be helping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params for LinearSVC\n",
      "{'C': 0.1, 'gamma': 10}\n",
      "\n",
      "train\n",
      "             S   SI   TA\n",
      "        S  676  460  310\n",
      "       SI  450 1936  847\n",
      "       TA  360  913 2954\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          S       0.45      0.47      0.46      1446\n",
      "         SI       0.59      0.60      0.59      3233\n",
      "         TA       0.72      0.70      0.71      4227\n",
      "\n",
      "avg / total       0.63      0.62      0.63      8906\n",
      "\n",
      "test09\n",
      "             S   SI   TA\n",
      "        S  153   31   86\n",
      "       SI   61  148  130\n",
      "       TA   48  196  255\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          S       0.58      0.57      0.58       270\n",
      "         SI       0.39      0.44      0.41       339\n",
      "         TA       0.54      0.51      0.53       499\n",
      "\n",
      "avg / total       0.51      0.50      0.50      1108\n",
      "\n",
      "test10\n",
      "             S   SI   TA\n",
      "        S  219   12    2\n",
      "       SI  118  303   52\n",
      "       TA   37   93  232\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          S       0.59      0.94      0.72       233\n",
      "         SI       0.74      0.64      0.69       473\n",
      "         TA       0.81      0.64      0.72       362\n",
      "\n",
      "avg / total       0.73      0.71      0.70      1068\n",
      "\n",
      "test11\n",
      "             S   SI   TA\n",
      "        S  173  307   65\n",
      "       SI   40  260    7\n",
      "       TA   77   91  129\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          S       0.60      0.32      0.41       545\n",
      "         SI       0.40      0.85      0.54       307\n",
      "         TA       0.64      0.43      0.52       297\n",
      "\n",
      "avg / total       0.55      0.49      0.47      1149\n",
      "\n",
      "testall\n",
      "             S   SI   TA\n",
      "        S  545  350  153\n",
      "       SI  219  711  189\n",
      "       TA  162  380  616\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          S       0.59      0.52      0.55      1048\n",
      "         SI       0.49      0.64      0.56      1119\n",
      "         TA       0.64      0.53      0.58      1158\n",
      "\n",
      "avg / total       0.58      0.56      0.56      3325\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "# use grid search to find best hyperparameters\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'C': [0.1, 0.5, 1], 'gamma': [10, 20, 30]}]\n",
    "\n",
    "classifier = GridSearchCV(svm.SVC(class_weight='auto'), tuned_parameters)\n",
    "classifier.fit(train_sklearn.X, train_sklearn.y)\n",
    "\n",
    "print(\"Best params for SVC with rbf kernel\")\n",
    "print(classifier.best_params_)\n",
    "print()\n",
    "\n",
    "train_predictions = classifier.predict(train_sklearn.X)\n",
    "kinecture.report_accuracy(train_predictions, train_sklearn.y, header=\"train\")\n",
    "\n",
    "test09_predictions = classifier.predict(test09_sklearn.X)\n",
    "kinecture.report_accuracy(test09_predictions, test09_sklearn.y, header=\"test09\")\n",
    "\n",
    "test10_predictions = classifier.predict(test10_sklearn.X)\n",
    "kinecture.report_accuracy(test10_predictions, test10_sklearn.y, header=\"test10\")\n",
    "\n",
    "test11_predictions = classifier.predict(test11_sklearn.X)\n",
    "kinecture.report_accuracy(test11_predictions, test11_sklearn.y, header=\"test11\")\n",
    "\n",
    "testall_predictions = classifier.predict(testall_sklearn.X)\n",
    "kinecture.report_accuracy(testall_predictions, testall_sklearn.y, header=\"testall\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-235-1bdda9ea86c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m                                    test_size=0.2, random_state=0)\n\u001b[1;32m     82\u001b[0m \u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinearSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m \u001b[0mplot_learning_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-235-1bdda9ea86c6>\u001b[0m in \u001b[0;36mplot_learning_curve\u001b[0;34m(estimator, title, X, y, ylim, cv, n_jobs, train_sizes)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Score\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     train_sizes, train_scores, test_scores = learning_curve(\n\u001b[0;32m---> 49\u001b[0;31m         estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0mtrain_scores_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mtrain_scores_std\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/site-packages/sklearn/learning_curve.py\u001b[0m in \u001b[0;36mlearning_curve\u001b[0;34m(estimator, X, y, train_sizes, cv, scoring, exploit_incremental_learning, n_jobs, pre_dispatch, verbose)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn_train_samples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             verbose, parameters=None, fit_params=None, return_train_score=True)\n\u001b[0;32m--> 137\u001b[0;31m             for train, test in cv for n_train_samples in train_sizes_abs)\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mn_cv_folds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mn_unique_ticks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    664\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 666\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    667\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    530\u001b[0m                             \u001b[0;31m# We can now allow subprocesses again\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m                             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'__JOBLIB_SPAWNED_PARALLEL__'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m                     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTransportableException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m                         \u001b[0;31m# Capture exception to add information on the local\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python3/3.4.3/Frameworks/Python.framework/Versions/3.4/lib/python3.4/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python3/3.4.3/Frameworks/Python.framework/Versions/3.4/lib/python3.4/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python3/3.4.3/Frameworks/Python.framework/Versions/3.4/lib/python3.4/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python3/3.4.3/Frameworks/Python.framework/Versions/3.4/lib/python3.4/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn import cross_validation\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.datasets import load_digits\n",
    "# from sklearn.learning_curve import learning_curve\n",
    "\n",
    "\n",
    "# def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "#                         n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "#     \"\"\"\n",
    "#     Generate a simple plot of the test and traning learning curve.\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     estimator : object type that implements the \"fit\" and \"predict\" methods\n",
    "#         An object of that type which is cloned for each validation.\n",
    "\n",
    "#     title : string\n",
    "#         Title for the chart.\n",
    "\n",
    "#     X : array-like, shape (n_samples, n_features)\n",
    "#         Training vector, where n_samples is the number of samples and\n",
    "#         n_features is the number of features.\n",
    "\n",
    "#     y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "#         Target relative to X for classification or regression;\n",
    "#         None for unsupervised learning.\n",
    "\n",
    "#     ylim : tuple, shape (ymin, ymax), optional\n",
    "#         Defines minimum and maximum yvalues plotted.\n",
    "\n",
    "#     cv : integer, cross-validation generator, optional\n",
    "#         If an integer is passed, it is the number of folds (defaults to 3).\n",
    "#         Specific cross-validation objects can be passed, see\n",
    "#         sklearn.cross_validation module for the list of possible objects\n",
    "\n",
    "#     n_jobs : integer, optional\n",
    "#         Number of jobs to run in parallel (default 1).\n",
    "#     \"\"\"\n",
    "#     plt.figure()\n",
    "#     plt.title(title)\n",
    "#     if ylim is not None:\n",
    "#         plt.ylim(*ylim)\n",
    "#     plt.xlabel(\"Training examples\")\n",
    "#     plt.ylabel(\"Score\")\n",
    "#     train_sizes, train_scores, test_scores = learning_curve(\n",
    "#         estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "#     train_scores_mean = np.mean(train_scores, axis=1)\n",
    "#     train_scores_std = np.std(train_scores, axis=1)\n",
    "#     test_scores_mean = np.mean(test_scores, axis=1)\n",
    "#     test_scores_std = np.std(test_scores, axis=1)\n",
    "#     plt.grid()\n",
    "\n",
    "#     plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "#                      train_scores_mean + train_scores_std, alpha=0.1,\n",
    "#                      color=\"r\")\n",
    "#     plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "#                      test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "#     plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "#              label=\"Training score\")\n",
    "#     plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "#              label=\"Cross-validation score\")\n",
    "\n",
    "#     plt.legend(loc=\"best\")\n",
    "#     return plt\n",
    "\n",
    "# X, y = train_sklearn.X, train_sklearn.y\n",
    "\n",
    "\n",
    "# title = \"Learning Curves (Naive Bayes)\"\n",
    "# # Cross validation with 100 iterations to get smoother mean test and train\n",
    "# # score curves, each time with 20% data randomly selected as a validation set.\n",
    "# cv = cross_validation.ShuffleSplit(X.shape[0], n_iter=100,\n",
    "#                                    test_size=0.2, random_state=0)\n",
    "\n",
    "# title = \"Learning Curves (SVM, RBF kernel, $\\gamma=0.001$)\"\n",
    "# # SVC is more expensive so we do a lower number of CV iterations:\n",
    "# cv = cross_validation.ShuffleSplit(X.shape[0], n_iter=10,\n",
    "#                                    test_size=0.2, random_state=0)\n",
    "# estimator = svm.LinearSVC()\n",
    "# plot_learning_curve(estimator, title, X, y, (0.7, 1.01), cv=cv, n_jobs=4)\n",
    "\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
